{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt\n"
     ]
    }
   ],
   "source": [
    "%cd lit-gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/aniketmaurya/b8e5bd3f1594bd31ed34375ed916f075/raw/b831e5ab054ac6c94a4a409d32be14b6fdcad82a/databricks-dolly-15k.csv\n",
    "# !mv databricks-dolly-15k.csv /data/aniket/datasets/databricks-dolly-15k.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/data/aniket/datasets/databricks-dolly-15k.csv\").replace(np.NaN, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/data/aniket/datasets/databricks-dolly-15k.csv\").replace(np.NaN, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which is a species of fish? Tope or Rope</td>\n",
       "      <td></td>\n",
       "      <td>Tope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why can camels survive for long without water?</td>\n",
       "      <td></td>\n",
       "      <td>Camels use the fat in their humps to keep them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alice's parents have three daughters: Amy, Jes...</td>\n",
       "      <td></td>\n",
       "      <td>The name of the third daughter is Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When was Tomoaki Komorida born?</td>\n",
       "      <td>Komorida was born in Kumamoto Prefecture on Ju...</td>\n",
       "      <td>Tomoaki Komorida was born on July 10,1981.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         instruction  \\\n",
       "0         When did Virgin Australia start operating?   \n",
       "1           Which is a species of fish? Tope or Rope   \n",
       "2     Why can camels survive for long without water?   \n",
       "3  Alice's parents have three daughters: Amy, Jes...   \n",
       "4                    When was Tomoaki Komorida born?   \n",
       "\n",
       "                                               input  \\\n",
       "0  Virgin Australia, the trading name of Virgin A...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  Komorida was born in Kumamoto Prefecture on Ju...   \n",
       "\n",
       "                                              output  \n",
       "0  Virgin Australia commenced services on 31 Augu...  \n",
       "1                                               Tope  \n",
       "2  Camels use the fat in their humps to keep them...  \n",
       "3            The name of the third daughter is Alice  \n",
       "4         Tomoaki Komorida was born on July 10,1981.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"instruction\", \"input\", \"output\"]\n",
    "df[COLUMNS].to_csv(\"databricks-dolly-15k.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "train has 13,510 samples\n",
      "test has 1,501 samples\n",
      "Processing train split ...\n",
      "100%|███████████████████████████████████| 13510/13510 [00:11<00:00, 1136.64it/s]\n",
      "Processing test split ...\n",
      "100%|█████████████████████████████████████| 1501/1501 [00:01<00:00, 1186.15it/s]\n"
     ]
    }
   ],
   "source": [
    "!python scripts/prepare_csv.py \\\n",
    "    --csv_path \"databricks-dolly-15k.csv\" \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --destination_path \"data/dolly\" \\\n",
    "    --max_seq_length 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "{'eval_interval': 100, 'save_interval': 100, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 4, 'gradient_accumulation_iters': 32, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
      "Seed set to 1337\n",
      "Loading model '/data/aniket/Llama-2-7b-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 11008, 'rope_condense_ratio': 1, 'rope_base': 10000, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Number of trainable parameters: 4,194,304\n",
      "Number of non trainable parameters: 6,738,415,616\n",
      "Seed set to 1337\n",
      "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      "I recommend The Little Prince because, in my opinion, it is one of the most thoughtful and original movies ever made. The Little Prince speaks to us on a profound level because it deals with the question of what makes life worth living and how we can remain open to the world. The Little Prince is a timeless classic that is sure to appeal to your interests.\n",
      "\n",
      "### Explanation:\n",
      "* The Little Prince is a visually stunning film that\n",
      "Estimated TFLOPs: 617.94\n",
      "Measured TFLOPs: 55.63\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 335, in <module>\n",
      "    CLI(setup)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 96, in CLI\n",
      "    return _run_component(components, cfg_init)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 181, in _run_component\n",
      "    return component(**cfg)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 96, in setup\n",
      "    fabric.launch(main, data_dir, checkpoint_dir, out_dir)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 153, in main\n",
      "    train(fabric, model, optimizer, scheduler, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 216, in train\n",
      "    logits = model(input_ids, lm_head_chunk_size=128)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/wrappers.py\", line 121, in forward\n",
      "    output = self._forward_module(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/lora.py\", line 498, in forward\n",
      "    x = block(x, cos, sin, mask, input_pos)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/model.py\", line 169, in forward\n",
      "    x = x + self.mlp(self.norm_2(x))\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/rmsnorm.py\", line 20, in forward\n",
      "    x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 39.39 GiB of which 32.12 MiB is free. Including non-PyTorch memory, this process has 39.36 GiB memory in use. Of the allocated memory 38.66 GiB is allocated by PyTorch, and 188.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python finetune/lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --data_dir \"data/dolly\" \\\n",
    "    --out_dir \"out/lora/dolly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with `bf16` Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_interval': 100, 'save_interval': 100, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 4, 'gradient_accumulation_iters': 32, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
      "Seed set to 1337\n",
      "Loading model '/data/aniket/Llama-2-7b-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 11008, 'rope_condense_ratio': 1, 'rope_base': 10000, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Number of trainable parameters: 4,194,304\n",
      "Number of non trainable parameters: 6,738,415,616\n",
      "Seed set to 1337\n",
      "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      "I recommend The Little Prince because it is an animated movie based on one of the most famous novels in history. The movie is very sweet and speaks about how to be adult and live life to the fullest. The Little Prince movie is a great way to bond with someone and become closer to them.\n",
      " use Test::More;\n",
      "\n",
      "use_ok( 'JSON::XS' );\n",
      "\n",
      "ok( my $obj = JSON::XS->new->utf\n",
      "Estimated TFLOPs: 617.94\n",
      "Measured TFLOPs: 55.63\n",
      "iter 0 step 0: loss 0.9183, iter time: 463.79ms\n",
      "iter 1 step 0: loss 1.1069, iter time: 251.20ms\n",
      "iter 2 step 0: loss 0.9962, iter time: 241.01ms\n",
      "iter 3 step 0: loss 0.7540, iter time: 345.42ms\n",
      "iter 4 step 0: loss 1.2208, iter time: 335.51ms\n",
      "iter 5 step 0: loss 0.9043, iter time: 241.95ms\n",
      "iter 6 step 0: loss 0.7237, iter time: 316.86ms\n",
      "iter 7 step 0: loss 0.9042, iter time: 335.76ms\n",
      "iter 8 step 0: loss 0.8951, iter time: 334.97ms\n",
      "iter 9 step 0: loss 1.2022, iter time: 161.05ms\n",
      "iter 10 step 0: loss 0.7890, iter time: 334.97ms\n",
      "iter 11 step 0: loss 0.7724, iter time: 337.16ms\n",
      "iter 12 step 0: loss 1.2007, iter time: 307.53ms\n",
      "iter 13 step 0: loss 0.8398, iter time: 335.26ms\n",
      "iter 14 step 0: loss 0.8229, iter time: 237.67ms\n",
      "iter 15 step 0: loss 1.5182, iter time: 137.95ms\n",
      "iter 16 step 0: loss 1.3686, iter time: 193.73ms\n",
      "iter 17 step 0: loss 0.8808, iter time: 335.61ms\n",
      "iter 18 step 0: loss 1.9538, iter time: 132.75ms\n",
      "iter 19 step 0: loss 1.2983, iter time: 205.64ms\n",
      "iter 20 step 0: loss 1.1093, iter time: 251.84ms\n",
      "iter 21 step 0: loss 1.8022, iter time: 168.16ms\n",
      "iter 22 step 0: loss 1.9984, iter time: 131.28ms\n",
      "iter 23 step 0: loss 1.1035, iter time: 335.05ms\n",
      "iter 24 step 0: loss 1.2481, iter time: 306.74ms\n",
      "iter 25 step 0: loss 1.3748, iter time: 300.49ms\n",
      "iter 26 step 0: loss 0.9778, iter time: 336.12ms\n",
      "iter 27 step 0: loss 0.9154, iter time: 233.52ms\n",
      "iter 28 step 0: loss 0.7997, iter time: 302.76ms\n",
      "iter 29 step 0: loss 1.3308, iter time: 207.26ms\n",
      "iter 30 step 0: loss 0.6532, iter time: 337.51ms\n",
      "iter 31 step 1: loss 1.0829, iter time: 420.48ms (optimizer.step)\n",
      "iter 32 step 1: loss 1.3583, iter time: 140.69ms\n",
      "iter 33 step 1: loss 1.7057, iter time: 137.56ms\n",
      "iter 34 step 1: loss 0.8873, iter time: 335.59ms\n",
      "iter 35 step 1: loss 1.6984, iter time: 133.53ms\n",
      "iter 36 step 1: loss 1.0488, iter time: 335.17ms\n",
      "iter 37 step 1: loss 1.1402, iter time: 167.32ms\n",
      "iter 38 step 1: loss 1.5600, iter time: 193.89ms\n",
      "iter 39 step 1: loss 1.1162, iter time: 192.32ms\n",
      "iter 40 step 1: loss 1.1501, iter time: 191.73ms\n",
      "iter 41 step 1: loss 1.1204, iter time: 232.79ms\n",
      "iter 42 step 1: loss 0.7673, iter time: 238.49ms\n",
      "iter 43 step 1: loss 1.4620, iter time: 159.94ms\n",
      "iter 44 step 1: loss 1.0607, iter time: 251.81ms\n",
      "iter 45 step 1: loss 1.7463, iter time: 138.28ms\n",
      "iter 46 step 1: loss 1.3248, iter time: 132.98ms\n",
      "iter 47 step 1: loss 1.1639, iter time: 141.31ms\n",
      "iter 48 step 1: loss 0.9796, iter time: 208.30ms\n",
      "iter 49 step 1: loss 0.9902, iter time: 202.26ms\n",
      "iter 50 step 1: loss 1.0382, iter time: 336.95ms\n",
      "iter 51 step 1: loss 0.8437, iter time: 335.15ms\n",
      "iter 52 step 1: loss 0.9786, iter time: 189.48ms\n",
      "iter 53 step 1: loss 1.3225, iter time: 249.64ms\n",
      "iter 54 step 1: loss 1.4890, iter time: 135.88ms\n",
      "iter 55 step 1: loss 1.0587, iter time: 196.35ms\n",
      "iter 56 step 1: loss 1.6266, iter time: 168.02ms\n",
      "iter 57 step 1: loss 1.1368, iter time: 213.42ms\n",
      "iter 58 step 1: loss 1.3256, iter time: 138.71ms\n",
      "iter 59 step 1: loss 0.7587, iter time: 334.84ms\n",
      "iter 60 step 1: loss 0.7072, iter time: 334.75ms\n",
      "iter 61 step 1: loss 0.7073, iter time: 234.02ms\n",
      "iter 62 step 1: loss 1.2410, iter time: 177.50ms\n",
      "iter 63 step 2: loss 1.6647, iter time: 192.11ms (optimizer.step)\n",
      "iter 64 step 2: loss 1.4434, iter time: 131.94ms\n",
      "iter 65 step 2: loss 0.9476, iter time: 230.59ms\n",
      "iter 66 step 2: loss 1.4008, iter time: 177.87ms\n",
      "iter 67 step 2: loss 0.9420, iter time: 209.46ms\n",
      "iter 68 step 2: loss 0.8676, iter time: 253.22ms\n",
      "iter 69 step 2: loss 0.8046, iter time: 335.26ms\n",
      "iter 70 step 2: loss 1.1349, iter time: 271.28ms\n",
      "iter 71 step 2: loss 0.9039, iter time: 272.76ms\n",
      "iter 72 step 2: loss 0.6269, iter time: 335.04ms\n",
      "iter 73 step 2: loss 1.4143, iter time: 133.84ms\n",
      "iter 74 step 2: loss 1.2474, iter time: 191.08ms\n",
      "iter 75 step 2: loss 0.8083, iter time: 335.58ms\n",
      "iter 76 step 2: loss 1.2763, iter time: 230.42ms\n",
      "iter 77 step 2: loss 1.4049, iter time: 160.15ms\n",
      "iter 78 step 2: loss 1.0233, iter time: 334.73ms\n",
      "iter 79 step 2: loss 0.5809, iter time: 335.22ms\n",
      "iter 80 step 2: loss 0.8390, iter time: 335.21ms\n",
      "iter 81 step 2: loss 1.0665, iter time: 335.53ms\n",
      "iter 82 step 2: loss 1.6996, iter time: 134.63ms\n",
      "iter 83 step 2: loss 0.7579, iter time: 334.82ms\n",
      "iter 84 step 2: loss 1.6947, iter time: 337.28ms\n",
      "iter 85 step 2: loss 0.5738, iter time: 335.41ms\n",
      "iter 86 step 2: loss 1.7991, iter time: 133.65ms\n",
      "iter 87 step 2: loss 1.0493, iter time: 334.84ms\n",
      "iter 88 step 2: loss 1.0444, iter time: 253.92ms\n",
      "iter 89 step 2: loss 0.9102, iter time: 336.63ms\n",
      "iter 90 step 2: loss 0.7015, iter time: 235.85ms\n",
      "iter 91 step 2: loss 1.8866, iter time: 139.06ms\n"
     ]
    }
   ],
   "source": [
    "!python finetune/lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --data_dir \"data/dolly\" \\\n",
    "    --out_dir \"out/lora/dolly\" \\\n",
    "    --precision bf16-true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with `bf16` and 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_interval': 100, 'save_interval': 100, 'eval_iters': 100, 'eval_max_new_tokens': 100, 'log_interval': 1, 'devices': 1, 'learning_rate': 0.0003, 'batch_size': 128, 'micro_batch_size': 4, 'gradient_accumulation_iters': 32, 'max_iters': 50000, 'weight_decay': 0.01, 'lora_r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'lora_query': True, 'lora_key': False, 'lora_value': True, 'lora_projection': False, 'lora_mlp': False, 'lora_head': False, 'warmup_steps': 100}\n",
      "Seed set to 1337\n",
      "Loading model '/data/aniket/Llama-2-7b-hf/lit_model.pth' with {'org': 'meta-llama', 'name': 'Llama-2-7b-hf', 'block_size': 4096, 'vocab_size': 32000, 'padding_multiple': 64, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 32, 'shared_attention_norm': False, '_norm_class': 'RMSNorm', 'norm_eps': 1e-05, '_mlp_class': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 11008, 'rope_condense_ratio': 1, 'rope_base': 10000, 'r': 8, 'alpha': 16, 'dropout': 0.05, 'to_query': True, 'to_key': False, 'to_value': True, 'to_projection': False, 'to_mlp': False, 'to_head': False, 'head_size': 128, 'rope_n_elem': 128}\n",
      "Number of trainable parameters: 4,194,304\n",
      "Number of non trainable parameters: 6,738,415,616\n",
      "Seed set to 1337\n",
      "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 4096\n",
      "Validating ...\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Recommend a movie for me to watch during the weekend and explain the reason.\n",
      "\n",
      "### Response:\n",
      "Since you were very keen to know more about china, I would like to recommend a movie to you for your weekend. The name of the movie is \"The Shining\".\n",
      "\n",
      "The plot is about a man struggling to maintain his own and his family's sanity as he moves into a haunted hotel and confronts the horrors hidden within the hotel. The movie is very interesting and it is full of mysteries and puzzles .In my opinion, this movie is\n",
      "Estimated TFLOPs: 617.94\n",
      "Measured TFLOPs: 55.63\n",
      "iter 0 step 0: loss 0.9694, iter time: 572.92ms\n",
      "iter 1 step 0: loss 1.1576, iter time: 344.16ms\n",
      "iter 2 step 0: loss 1.0533, iter time: 333.32ms\n",
      "iter 3 step 0: loss 0.7949, iter time: 443.43ms\n",
      "iter 4 step 0: loss 1.2590, iter time: 433.73ms\n",
      "iter 5 step 0: loss 0.9482, iter time: 333.87ms\n",
      "iter 6 step 0: loss 0.7669, iter time: 412.59ms\n",
      "iter 7 step 0: loss 0.9375, iter time: 433.81ms\n",
      "iter 8 step 0: loss 0.9333, iter time: 436.30ms\n",
      "iter 9 step 0: loss 1.2729, iter time: 251.64ms\n",
      "iter 10 step 0: loss 0.8294, iter time: 433.18ms\n",
      "iter 11 step 0: loss 0.8089, iter time: 435.02ms\n",
      "iter 12 step 0: loss 1.2594, iter time: 404.82ms\n",
      "iter 13 step 0: loss 0.9106, iter time: 432.84ms\n",
      "iter 14 step 0: loss 0.8736, iter time: 329.56ms\n",
      "iter 15 step 0: loss 1.6365, iter time: 222.19ms\n",
      "iter 16 step 0: loss 1.4289, iter time: 288.63ms\n",
      "iter 17 step 0: loss 0.9150, iter time: 432.96ms\n",
      "iter 18 step 0: loss 2.0450, iter time: 203.23ms\n",
      "iter 19 step 0: loss 1.3516, iter time: 299.34ms\n",
      "iter 20 step 0: loss 1.1566, iter time: 344.56ms\n",
      "iter 21 step 0: loss 1.8709, iter time: 258.90ms\n",
      "iter 22 step 0: loss 2.1531, iter time: 189.21ms\n",
      "iter 23 step 0: loss 1.1481, iter time: 432.90ms\n",
      "iter 24 step 0: loss 1.2976, iter time: 404.29ms\n",
      "iter 25 step 0: loss 1.4265, iter time: 397.24ms\n",
      "iter 26 step 0: loss 1.0357, iter time: 434.01ms\n",
      "iter 27 step 0: loss 0.9648, iter time: 325.44ms\n",
      "iter 28 step 0: loss 0.8463, iter time: 400.29ms\n",
      "iter 29 step 0: loss 1.3807, iter time: 300.72ms\n",
      "iter 30 step 0: loss 0.6870, iter time: 433.20ms\n",
      "iter 31 step 1: loss 1.1326, iter time: 496.24ms (optimizer.step)\n",
      "iter 32 step 1: loss 1.4475, iter time: 207.98ms\n",
      "iter 33 step 1: loss 1.7841, iter time: 209.89ms\n",
      "iter 34 step 1: loss 0.9387, iter time: 432.48ms\n",
      "iter 35 step 1: loss 1.7852, iter time: 205.91ms\n",
      "iter 36 step 1: loss 1.0877, iter time: 433.25ms\n",
      "iter 37 step 1: loss 1.2030, iter time: 258.32ms\n",
      "iter 38 step 1: loss 1.6152, iter time: 287.46ms\n",
      "iter 39 step 1: loss 1.1704, iter time: 286.59ms\n",
      "iter 40 step 1: loss 1.2057, iter time: 284.29ms\n",
      "iter 41 step 1: loss 1.1742, iter time: 325.19ms\n",
      "iter 42 step 1: loss 0.8054, iter time: 331.13ms\n",
      "iter 43 step 1: loss 1.5284, iter time: 249.31ms\n",
      "iter 44 step 1: loss 1.1063, iter time: 344.54ms\n",
      "iter 45 step 1: loss 1.8367, iter time: 219.53ms\n",
      "iter 46 step 1: loss 1.4100, iter time: 208.06ms\n",
      "iter 47 step 1: loss 1.2347, iter time: 223.75ms\n",
      "iter 48 step 1: loss 1.0268, iter time: 300.69ms\n",
      "iter 49 step 1: loss 1.0434, iter time: 295.40ms\n",
      "iter 50 step 1: loss 1.0815, iter time: 434.01ms\n",
      "iter 51 step 1: loss 0.9039, iter time: 433.87ms\n",
      "iter 52 step 1: loss 1.0368, iter time: 282.67ms\n",
      "iter 53 step 1: loss 1.3897, iter time: 342.66ms\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 335, in <module>\n",
      "    CLI(setup)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 96, in CLI\n",
      "    return _run_component(components, cfg_init)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/jsonargparse/_cli.py\", line 181, in _run_component\n",
      "    return component(**cfg)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 96, in setup\n",
      "    fabric.launch(main, data_dir, checkpoint_dir, out_dir)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 834, in launch\n",
      "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 920, in _wrap_and_launch\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/fabric.py\", line 925, in _wrap_with_setup\n",
      "    return to_run(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 153, in main\n",
      "    train(fabric, model, optimizer, scheduler, train_data, val_data, checkpoint_dir, out_dir, speed_monitor)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/finetune/lora.py\", line 216, in train\n",
      "    logits = model(input_ids, lm_head_chunk_size=128)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/lightning/fabric/wrappers.py\", line 121, in forward\n",
      "    output = self._forward_module(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/lora.py\", line 498, in forward\n",
      "    x = block(x, cos, sin, mask, input_pos)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/model.py\", line 158, in forward\n",
      "    h = self.attn(n_1, cos, sin, mask, input_pos)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/model.py\", line 196, in forward\n",
      "    qkv = self.attn(x)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/aniket/miniconda3/envs/am/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/lora.py\", line 385, in forward\n",
      "    lora = self.zero_pad(after_B) * self.scaling  # (64, 64, 256) after zero_pad (64, 64, 384)\n",
      "  File \"/home/aniket/workspace/aniket-codes/2023-10-24 MLOps World/lit-gpt/lit_gpt/lora.py\", line 294, in zero_pad\n",
      "    1, torch.tensor(self.lora_ind, device=result.device), x.reshape(-1, sum(self.qkv_shapes))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python finetune/lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --data_dir \"data/dolly\" \\\n",
    "    --out_dir \"out/lora/dolly\" \\\n",
    "    --precision bf16-true \\\n",
    "    --quantize bnb.fp4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/merge_lora.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --lora_path \"out/dolly/Llama-2-7b-hf/lit_model_lora_finetuned.pth\" \\\n",
    "    --out_dir \"out/dolly/Llama-2-7b-hf/\"\n",
    "\n",
    "\n",
    "!python eval/lm_eval_harness.py \\\n",
    "    --checkpoint_dir \"/data/aniket/Llama-2-7b-hf\" \\\n",
    "    --eval_tasks \"[truthfulqa_mc]\" \\\n",
    "    --precision \"bf16-true\" \\\n",
    "    --batch_size 4 \\\n",
    "    --save_filepath \"results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
